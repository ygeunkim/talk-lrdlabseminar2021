---
title: "R for Statistics Research"
author: "Young Geun Kim^[[ygeunkim.github.io](https://ygeunkim.github.io)]"
institute: "[Department of Statistics](https://stat.skku.edu/stat/index.jsp)"
date: "`r format(as.Date('2021-10-25'), '%d %b, %Y')` (updated: `r format(Sys.Date(), '%d %b, %Y')`)"
bibliography: "../docs/latex/talkref.bib"
biblio-style: "apalike"
output:
  bookdown::beamer_presentation2:
    toc: yes
    slide_level: 2
    theme: "Antibes"
    colortheme: "beaver"
    fonttheme: "default"
    citation_package: natbib
    latex_engine: lualatex
    includes:
      in_header: "../docs/latex/preamble.tex"
knit:
  (function(inputFile, encoding) {
    rmarkdown::render(input = inputFile, encoding = encoding, output_dir = "../output/slides")
  })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  collapse = TRUE,
  out.width = "70%",
  fig.align = "center",
  fig.width = 6,
  fig.pos = "H",
  fig.asp = .618
)
knitr::knit_hooks$set(
  document = function(x) {
    sub("\\usepackage[]{color}", "\\usepackage{xcolor}", x, fixed = TRUE)
  }
)
options(digits = 3)
options(knitr.kable.NA = "")
options(kableExtra.latex.load_packages = FALSE)
```

```{r pkg, include=FALSE, message=FALSE}
library(tidyverse)
library(data.table)
library(foreach)
library(forecast)
```


# Introductory R Functions

## Class - Vector

```{r}
(x <- c(2.1, 4.2, 3.3, 5.4))
class(x)
```

- Vectorized code gives the fast result
- `rowSums()`, `colSums()`, `rowMeans()`, `colMeans()` are faster than `apply()` [@Wickham:2019vd] \emoji{face-with-monocle}

## Class - Data frame

\tiny
```{r}
(df <- data.frame(x = 1:2, y = 2:1, z = letters[1:2]))
class(df)
```
\normalsize

- Data for data analysis might be data frame
- Indexing in data frame connects to data transformation

\tiny
```{r, size='tiny'}
y <- MASS::Boston[, c("medv", "lstat", "age")]
head(y)
```
\normalsize

## Data Analysis using data frame

Multiple linear regression for `medv` ~ `lstat` + `age`:

```{r}
lm(medv ~ ., data = y)
```

- Data frame is proper to use with many `R` model functions.

## Class - List

Contains any object in each element:

```{r}
(z <- list(a = x, b = df))
class(z)
```

## Other Classes

- `matrix` and `array`
    - 2d matrix: linear algebra
    - more than 2d: used in deep learning ("tensor")
- `factor`
- Date: `POSIXct`, `POSIXt`, etc
- Time series: `ts`

```{r}
ts(1:10, frequency = 4, start = c(1959, 2))
```

## Time Series Model

\tiny
```{r}
lh
class(lh)
```
\normalsize

Fit AR(1) using `arima` function:

\tiny
```{r, size='tiny'}
arima(lh, order = c(1,0,0))
```
\normalsize

## Tidy Data

```{r tidyfig, echo=FALSE, fig.cap="Tidy Data", out.width="50%"}
knitr::include_graphics("../docs/images/tidy-1.png")
```

- For easier data analysis, @JSSv059i10 suggests tidy representation of dataset, called **tidy data**.
    1. Each variable must have its own column.
    2. Each observation must have its own row.
    3. Each value must have its own cell.
- `tibble` package

## `read.csv` and `readr::read_csv`

- `read.csv(file)`: default function to import csv file
- `readr::read_csv(file)`: read csv file to `tibble`
    - Use this function \emoji{smiling-face-with-sunglasses}

## Data Wrangling

```{r wranglefig, echo=FALSE, fig.cap="Steps of Data Analysis"}
knitr::include_graphics("../docs/images/wrangle.png")
```

- Since many datasets are not tidy, we need data wrangling step
- `dplyr` provides data manipulation functions
- `tidyr` package does this

## Tidy Data

```{r}
tidyr::table1
```

## `dplyr`

- `filter()` picks observations using logical conditions
- `arrange()` reorders the rows
- `select()` picks variables by column names
- `mutates()` creates new variables with functions
- `summarise()` collapses many values to a single value, i.e. summary statistic

## Example

`group_by()`: excecute function for each group:

```{r}
tidyr::table1 %>% 
  group_by(country) %>% 
  summarise(
    average = mean(population), 
    std = sd(population)
  )
```

## Non-tidy Data 1

```{r}
tidyr::table4a
```

- values (year) are in variable (`1999`, `2000`)
- How to tidy? Gather these two variables into one variable.

## Non-tidy Data 2

\tiny
```{r}
tidyr::table2
```
\normalsize

- Looking at `type`, each observation does not have its own row.
- Then spread `type` taking values of `count`

## `tidyr`

`tidyr` package can tidy data-set. From @Wickham:r4ds,

- `pivot_longer()`: Figure \@ref(fig:longfig)
- `pivot_wider()`: Figure \@ref(fig:widefig)

::: columns

:::: column
```{r longfig, echo=FALSE, fig.cap="Gather", out.width="90%"}
knitr::include_graphics("../docs/images/long.png")
```
::::

:::: column
```{r widefig, echo=FALSE, fig.cap="Spread", out.width="90%"}
knitr::include_graphics("../docs/images/wide.png")
```
::::

:::

## Large Data

- When data file is too large
- `data.table` package focuses on memory optimization
- `read.csv` or `read_csv` can mistakenly read string as factor
- but `data.table::fread()` do not

## Visualization

| `ggplot2` | base plot |  
|:---------:|:---------:|  
|grammar of graphics|pen on paper model|  

### `ggplot2`

- Data: data frame (`mpg`)
- Aesthetic mapping: `aes()`
    - x-axis: `displ`
    - y-axis: `hwy`
- Layers: `geom_*()` function
    - scatter plot: `geom_point()`
    - smoothing: `geom_smooth()`

## Example

```{r ggplotfig, fig.cap="ggplot example"}
ggplot(mpg, aes(x = displ, y = hwy)) +
  geom_point() +
  geom_smooth() +
  theme_minimal()
```

## \emoji{star-struck} Simulation \emoji{star-struck}

- Monte Carlo simulation (parametric bootstrap)
- Bootstrap

## Monte Carlo Simulation

```{r simulfig, echo=FALSE, fig.cap="Simulating Our Model"}
knitr::include_graphics("../docs/images/simul-01.png")
```

## Monte Carlo Simulation

::: {.example #mcex name="Any quantity of interest"}
Suppose that $X_1, X_2 \iid N(0, 1)$. Estimate $\theta = \Ex \lvert X_1 - X_2 \rvert$.
:::

\begin{algorithm}[H] \label{alg:mcsimul}
  \SetAlgoLined
  \SetKwInOut{Input}{Input}
  \SetKwInOut{Output}{Output}
  \Input{distribution $f$, Number of iteration $M$}
  \For{$m \leftarrow 1$ \KwTo $M$}{
    Generate $(X_1^{(m)}, X_2^{(m)}) \iid N(0, 1)$\;
    Compute $\hat\theta^{(m)} = \lvert X_1^{(m)} - X_2^{(m)} \rvert$\;
  }
  Draw a histogram\;
  \Output{$\{ \hat\theta^{(1)}, \ldots, \hat\theta^{(M)} \}$}
  \caption{Empirical distribution of $\hat\theta$}
\end{algorithm}

## MC Estimates

- Recall if you took statistical computing (STA5011) course \emoji{beaming-face-with-smiling-eyes}
- MC estimator: $\bar{\hat\theta} = \frac{1}{M} \sum\limits_{m = 1}^M\hat\theta_m^{(m)}$
- MC standard error: $\widehat{SE}(\hat\theta) = \sqrt{\frac{1}{M - 1}\sum\limits_{m = 1}^M(\hat\theta^{(m)} - \bar{\hat\theta})}$
- MSE
- CI

## `R` programming for Example \@ref(exm:mcex)

- Generation can be done by matrix
- Compuation can be vectorized

```{r, size='tiny'}
# MC setting-------------------------
num_mc <- 2
num_iter <- 1000
set.seed(1)
# X1, X2 ~ N(0, 1)-------------------
data_mc <- 
  rnorm(num_mc * num_iter) %>% 
  matrix(ncol = 2, byrow = TRUE)
# theta = abs(data[,1] - data[,2])---
emp_theta <- abs(data_mc[,1] - data_mc[,2])
```

## Empirical distribution

```{r mcempfig, fig.cap="Empirical distribution of $\\hat\\theta$ for $\\lvert X_1 - X_2 \\rvert$"}
qplot(emp_theta, geom = "histogram", bins = 30) +
  theme_minimal() +
  xlab(expression(theta))
```

## Estimates

Estimator

```{r}
mean(emp_theta)
```

SE:

```{r}
sd(emp_theta)
```

## Bootstrap

```{r bootfig, echo=FALSE, fig.cap="Empirical Distribution of Bootstrap"}
knitr::include_graphics("../docs/images/simul-02.png")
```

## Bootstrap Algorithm

\begin{algorithm}[H] \label{alg:algboot}
  \SetAlgoLined
  \SetKwInOut{Input}{input}
  \SetKwInOut{Output}{output}
  \KwData{$n$ observations $x_1, \ldots, x_n$}
  \Input{statistic of interest $\hat\theta$, the number of bootstrap replicates $B$}
  \For{$b \leftarrow 1$ \KwTo $B$}{
    Sampling with replacement $X_{1}^{(b)}, \ldots, X_n^{(b)}$ from the observed sample\;
    Compute estimate $$\hat\theta(X_{1}^{(b)}, \ldots, X_n^{(b)}) \equiv \hat\theta_b^{\ast}$$\;
  }
  \Output{Bootstrap replications $\{ \hat\theta_1^\ast, \ldots, \hat\theta_B^\ast \}$}
  \caption{Bootstrap for $\hat\theta$}
\end{algorithm}

## Bootstrap Estimates

Bootstrap standard error:

$$\hat\sigma_B = \bigg[ \frac{1}{B - 1} \sum_{b = 1}^B (\hat\theta_b^{\ast} - \overline{\hat\theta^{\ast}})^2 \bigg]^{\frac{1}{2}}$$

where

$$\overline{\hat\theta^{\ast}} = \frac{1}{B} \sum\limits_{b = 1}^B \hat\theta_b^{\ast}$$

## In `R`

```{r}
resample <- function(x) {
  n <- length(x)
  # sampling with replacement---------------
  x[sample(1:n, size = n, replace = TRUE)] %>% 
    # estimator-----------------------------
    mean()
}
# example data------------------------------
unif_sample <- runif(50, max = 50)
```

## `for` loop

```{r}
resample_for <- function(x, B = 1000) {
  res <- numeric(B)
  for (b in 1:B) {
    res[b] <- resample(x = x)
  }
  res
}
```

## Result

```{r bootempfig, fig.cap="Bootstrap Replicates of Sample Mean", echo=FALSE}
set.seed(1)
qplot(
  resample_for(unif_sample, 1000),
  geom = "histogram", 
  bins = 30
) +
  theme_minimal() +
  xlab(expression(theta))
```

## Reproducible Documents

- Data can be changed while writing the document
- R Markdown helps reproducibility by integrating Markdown and `R`.
- Bookdown is a package for authoring books, but it also provides a function for single document: `bookdown::*_document2`
- See @Xie:2018ui and @Xie:2016uz.

## New Rmd File

```{r rmdfig1, echo=FALSE, fig.cap="Making R Markdown"}
knitr::include_graphics("../docs/images/rmd-01.png")
```

```{r rmdfig2, echo=FALSE, fig.cap="rmd Default"}
knitr::include_graphics("../docs/images/rmd-02.png")
```

## Style Guides

- Use `The tidyverse style guide`^[[https://style.tidyverse.org](https://style.tidyverse.org)]
- NOT `Google's R Style Guide`^[[https://google.github.io/styleguide/Rguide.html](https://google.github.io/styleguide/Rguide.html)]

## Project-Oriented Workflow

- [https://www.tidyverse.org/blog/2017/12/workflow-vs-script/](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/)
- Directory: the project folder
    - useful when sharing with other people
    - `renv` package: save every package in the project using into the project folder
- [https://www.r-bloggers.com/2018/08/structuring-r-projects/](https://www.r-bloggers.com/2018/08/structuring-r-projects/)
    - Use the efficient structure

# Parallel Computations

## Sources

- Books: 
    - @mccallum2012
    - @Matloff:2015wi
- Blogs:
    - @michael2018^[[https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html](https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html)]
    - @benito2021^[[https://www.blasbenito.com/post/02_parallelizing_loops_with_r/](https://www.blasbenito.com/post/02_parallelizing_loops_with_r/)]
    - @errickson2017^[[https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html#content](https://dept.stat.lsa.umich.edu/~jerrick/courses/stat701/notes/parallel.html#content)]
    - @statcompute2019^[[https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/](https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/)]

## Preliminaries

```{r parallelfig1, echo=FALSE, fig.cap="CPU", out.width="40%"}
knitr::include_graphics("../docs/images/parallel-01.png")
```

\footnotesize
- Node: Single computer consisting of a motherboard and OS.
- Processor = Socket: physical unit containing
- Core: the smallest computation unit of the processor
- Multi-core processor: Physical processor in which many cores are embedded
- Hardware thread = logical core: computation unit within a core that allows the core to do multiple things at once
- Worker: an independent process (i.e. computation and provides the result)
\normalsize

## Parallel Computing

- Parallel excecution: divide the queue among many workers that compute at the same times
- Parallel backend
    - The way of workers group execute the iterations of the order of the director node.
    - Provides the means for the director and workers to communicate, while allocating and managing the required computing resources.
    - **PSOCK and FORK**

## PSOCK

```{r psockfig, echo=FALSE, fig.cap="PSOCK backend"}
knitr::include_graphics("../docs/images/parallel-02.png")
```

- Cluster computing
- launch a new environment of R on each core

## FORK

```{r forkfig, echo=FALSE, fig.cap="FORK backend"}
knitr::include_graphics("../docs/images/parallel-03.png")
```

- Multi-core processor
- duplicate the entire current version of R and move it to a new core

## Difference

\small
| PSOCK (Parallel Socket Cluster) | FORK |  
|:------|:-----|  
|both in UNIX and windows | only in UNIX (Mac, Linux, etc) |  
|Environment of the director needs to be copied to one of each of worker (disadvantage) |Workers share the same environment: highly efficient |  
|Each socket works separately |Each parallel thread is a duplication of master process |  
|Copying is tricky |Implementing forking is easy |  
|Each process on each note is unique: cannot be cross-contaminated (advantage) |The fact that the processes are duplicates can cause issues, e.g. random number generation |  
\normalsize

FORK backend is much faster than PSOCK.^[[https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/](https://www.r-bloggers.com/2019/06/parallel-r-socket-or-fork/)]

## Packages

- `foreach`: Both PSOCK and FORK
- `parallel`:
    - `parLapply`, `parSapply`, and `parApply`: Both PSOCK and FORK as `foreach` does
    - `mclapply`, `pvec`: only FORK

## `foreach`

- loop with parallel
- `%do%`: non-parallel
- `%dopar%`: parallel
- `.combine`: combine the result
    - e.g. `.combine = rbind`, `.combind = c`
    - This makes the code useful

## Non-parallel `foreach` 1

```{r}
# without combine------
foreach(i = 1:2) %do% {
  i
}
# combine--------------
foreach(i = 1:2, .combine = c) %do% {
  i
}
```

## Non-parallel `foreach` 2

```{r}
foreach(i = 1:3, .combine = bind_rows) %do% {
  tibble(
    x = i,
    y = i + 1
  )
}
```

## Backend - Cluster

```{r}
parallel::detectCores()
n_cores <- parallel::detectCores() / 2
```

```{r}
(my_cluster <- parallel::makeCluster(n_cores))
```

## Backend - Register

```{r}
doParallel::registerDoParallel(cl = my_cluster)
```

You can check if the backends were registered well (this process is not required):

```{r}
foreach::getDoParRegistered()
foreach::getDoParWorkers()
foreach::getDoParName()
```

## Perform `foreach`

Use `%dopar%` instead of `%do%`:

```{r}
foreach(i = 1:2, .combine = c) %dopar% {
  i
}
```

When finished, stop the cluster:

```{r}
parallel::stopCluster(cl = my_cluster)
```

## Example \@ref(exm:mcex)

- `parallel::clusterEvalQ(cl, c(library(), library()))`: Import library for all workers (required)
    - Or, there is `.packages` argument in `foreach` function.
- `parallel::clusterExport(cl, c("object_name", "function_name"))`: Copies variables and functions to each worker

```{r}
my_cluster <- parallel::makeCluster(n_cores)
doParallel::registerDoParallel(cl = my_cluster)
parallel::clusterEvalQ(my_cluster, library(magrittr))
```

## Example \@ref(exm:mcex) - `foreach`

```{r}
set.seed(1)
theta_foreach <- foreach(b = 1:1000, .combine = c) %dopar% {
  resample(unif_sample)
}
# stop cluster---------------------
parallel::stopCluster(cl = my_cluster)
```

## Example \@ref(exm:mcex) - result

```{r bootempfig2, fig.cap="Bootstrap Replicates by foreach", echo=FALSE}
qplot(
  theta_foreach,
  geom = "histogram", 
  bins = 30
) +
  theme_minimal() +
  xlab(expression(theta))
```

## Forking for `foreach`

- Replace `parallel::makeCluster(n_cores)` with `parallel::makeForkCluster`
- `doMC::registerDoMC`

## `doMC::registerDoMC`

- This is quite simple
- Only one line is need
- Also, you can define function more easily.

\small
```{r}
resample_foreach <- function(x, B) {
  foreach(b = 1:B, .combine = c) %dopar% {
    resample(x)
  }
}
# implement--------------------------
doMC::registerDoMC(cores = n_cores)
theta_domc <- resample_foreach(unif_sample, 1000)
```
\normalsize

## `parLapply`, `parSapply`, and `parApply`

- Functionals
- Parallel backend like `foreach`

```{r}
my_cluster <- parallel::makeCluster(n_cores)
doParallel::registerDoParallel(cl = my_cluster)
parallel::clusterEvalQ(my_cluster, library(magrittr))
```

## Example \@ref(exm:mcex) - `parSapply`

```{r}
set.seed(1)
theta_apply <- parallel::parSapply(
  cl = my_cluster,
  1:1000,
  function(x) {
    resample(unif_sample)
  }
)
# stop cluster---------------------
parallel::stopCluster(cl = my_cluster)
```

## Example \@ref(exm:mcex) - result of `parSapply`

```{r parsapplyfig, fig.cap="Bootstrap Replicates by mclapply", echo=FALSE}
qplot(
  theta_apply,
  geom = "histogram", 
  bins = 30
) +
  theme_minimal() +
  xlab(expression(theta))
```

## `mclapply`

- `mclapply` is FORK backend
- Easy to implement
- `lapply`-way and additional `mc.cores` option

```{r}
set.seed(1)
theta_mcapply <- parallel::mclapply(
  1:1000,
  function(x) {
    resample(unif_sample)
  },
  mc.cores = n_cores
)
```

## Example \@ref(exm:mcex) - result of `mclapply`

```{r mclapplyfig, fig.cap="Bootstrap Replicates by parSapply", echo=FALSE}
qplot(
  theta_mcapply %>% unlist(),
  geom = "histogram", 
  bins = 30
) +
  theme_minimal() +
  xlab(expression(theta))
```

## `pvec`

- Vector map based on forking backend
- same as `sapply` option

## Reproducible Results

- `mclapply` result give not identical results by `set.seed()`
- It needs some methods
- See @mccallum2012 and [https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html](https://psu-psychology.github.io/r-bootcamp-2018/talks/parallel_r.html)

## Parallel Options in Popular Funtions

- `glmnet::cv.glmnet` has `parallel = FALSE` option.
    - Use `foreach`
- `boot` has `parallel = c("no", "multicore", "snow")`

## `Rcpp`

- Although parallelization helps performance of our codes, `Rcpp` might be basically the fastest in the `for` loop scheme
- Simple Gibbs sampler code in the official site: [https://gallery.rcpp.org/articles/gibbs-sampler/](https://gallery.rcpp.org/articles/gibbs-sampler/)

# R Packages

## Sources

- @Wickham:2015wq
- Recommend: Presentation of tidyverse team in last weak
    - [https://youtu.be/EpTkT6Rkgbs](https://youtu.be/EpTkT6Rkgbs)

## `devtools`

\tiny
```{r, eval=FALSE}
install.packages("devtools")
```
\normalsize

```{r packagefig1, echo=FALSE, fig.cap="R Package Project", out.width="50%"}
knitr::include_graphics("../docs/images/package-01.png")
```

## Workflow

1. Make function and load function
2. Metadata and license
3. Documentation of the function
4. Checking
5. Install
6. Testing

## `usethis`

- When write or make files, additional settings are required.
- `usethis` package do this instead of us
- `usethis::use_r()`: add function file

## Metadata

- DESCRIPTION file: Write about your package
- [https://github.com/tidyverse/dplyr/blob/master/DESCRIPTION](https://github.com/tidyverse/dplyr/blob/master/DESCRIPTION)
- use `usethis`
- `usethis::use_package()`: import other package
- `usethis::use_*_license("Name")` add Licence file and add this file, write in the description above license, and list into `.Rbuildignore`.
    - About license: [https://r-pkgs.org/license.html](https://r-pkgs.org/license.html)
    - MIT license is recommendable: `usethis::use_mit_license("Name")`

## Documentation

- Using `roxygen2` package, make help documents
- e.g. [https://r-pkgs.org/man.html](https://r-pkgs.org/man.html)
- Make `usethis::use_vignette("vignette-name")`

## Vignettes

- Long-form documentation
- e.g. [https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html](https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html)

## Build

- There is `build & reload` button in RStudio
- but it is not perfect
- vignettes are not build
- If you want vignettes in the package, use `devtools::build()`

## Checking

- Check button
- `devtools::check()`

## Testing

- [https://r-pkgs.org/tests.html](https://r-pkgs.org/tests.html)
- Using `testthat` package is easy
- `usethis::use_testthat()`

## Data

- Adding external dataset in the package: e.g. `MASS::Boston`
- [https://r-pkgs.org/data.html](https://r-pkgs.org/data.html)
- `usethis::use_data_raw()`
    - Code for dataset inside `data-raw` folder
    - add it into `.Rbuildignore`
- `usethis::use_data()`
    - Save data as `.Rdata` using this function
    - Use this in R script of `data-raw` folder

# Additional Tips

## `xlsx2csv`

- `xlsx` file: hard to deal with in `R`
    - `readxl` package
    - also has issue e.g. multiple header file \emoji{face-with-thermometer}
- Just change the file into `csv`
- [https://github.com/dilshod/xlsx2csv](https://github.com/dilshod/xlsx2csv)
- `xlsx2csv -s 3 -m NJClPFECA_SciHub_Data.xlsx ../processed/njcipeca.csv`

## Pipe

- `marittr` package in `tidyverse` family: `%>%`
    - RStudio shortcut: command + shift + m or ctrl + shift + m
- R default pipe operator from 4.1: `|>`
    - similar to `%>%`
    - but it cannot be referenced by `.` like `%>%`
    - e.g. `data %>% lm(y ~ x, data = .)`

## `kable` and `kableExtra`

- `knitr::kable`: make latex, html, markdown, or pandoc table in R markdown
- `kableExtra` package: build complex table based on `kable`.
    - e.g. multirow or multicol latex table
    - [https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html](https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html)

## Why `tidyverse`?

- [https://stackoverflow.com/questions/tagged/r](https://stackoverflow.com/questions/tagged/r)
- If you ask or look at the answer in the stackoverflow, people ask the questions with `tidyverse`
- `tidyverse` makes R much useful in data-processing \emoji{relieved-face}

## `tidyverts`

- `forecast`: Time series package made by Rob Hyndman
- Additionaly, `tidyverts` family was made by the team for tidy version of time series (`ts`)

<!-- cite -->
# Sources
